{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SA_Microsoft_Azure_Reinforcement_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "riv4L68dfNfC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e427c5a429e4a6b8c19db213894f93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd0f5a4db2974a749ff95639480e8afc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_282add3a1d92467ab7913cec0a7d9af8",
              "IPY_MODEL_9d8a8eb022a54aa6aa270b3eb61774db"
            ]
          }
        },
        "bd0f5a4db2974a749ff95639480e8afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "282add3a1d92467ab7913cec0a7d9af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4a939888489045ecb6ce704e80e3f016",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a8fa8cfa8334bc5af9e060087ff2119"
          }
        },
        "9d8a8eb022a54aa6aa270b3eb61774db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_98277269b41e4d02a06a670f5ea7701b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100000/100000 [10:05&lt;00:00, 165.03it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d91b613ca4f24eeca6c1f25822249624"
          }
        },
        "4a939888489045ecb6ce704e80e3f016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a8fa8cfa8334bc5af9e060087ff2119": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98277269b41e4d02a06a670f5ea7701b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d91b613ca4f24eeca6c1f25822249624": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E120gXTmfDbJ"
      },
      "source": [
        "# Introductions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVcMLDtZZ9FQ"
      },
      "source": [
        "## Reinforcment learning \n",
        "#### Reinforcement learning is the training of machine learning models to make a sequence of decisions. He agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation.\n",
        "#### Our system gets the state from the environment, based on this state, the system performs an action, Switching the environment to a new state, The environment gives some feedback.\n",
        "![](https://res.cloudinary.com/practicaldev/image/fetch/s--cEikblWi--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/iyqyvx6prnyfqzki3fna.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLWSOFKtcFRN"
      },
      "source": [
        "## Terminologies\n",
        "### Agent\n",
        "An agent takes actions; for example, a drone making a delivery, or Super Mario navigating a video game. The algorithm is the agent. In life, the agent is you.\n",
        "### Action (A)\n",
        "A is the set of all possible moves the agent can make. An action is almost self-explanatory, but it should be noted that agents usually choose from a list of discrete, possible actions. In video games, the list might include running right or left, jumping high or low, crouching or standing still. In the stock markets, the list might include buying, selling or holding any one of an array of securities and their derivatives. When handling aerial drones, alternatives would include many different velocities and accelerations in 3D space.\n",
        "### Discount factor\n",
        "The discount factor is multiplied by future rewards as discovered by the agent in order to dampen thse rewards’ effect on the agent’s choice of action. Why? It is designed to make future rewards worth less than immediate rewards; i.e. it enforces a kind of short-term hedonism in the agent. Often expressed with the lower-case Greek letter gamma: γ. If γ is .8, and there’s a reward of 10 points after 3 time steps, the present value of that reward is 0.8³ x 10. A discount factor of 1 would make future rewards worth just as much as immediate rewards. We’re fighting against delayed gratification here.\n",
        "### Environment\n",
        "The world through which the agent moves, and which responds to the agent. The environment takes the agent’s current state and action as input, and returns as output the agent’s reward and its next state. If you are the agent, the environment could be the laws of physics and the rules of society that process your actions and determine the consequences of them.\n",
        "### State (S)\n",
        "A state is a concrete and immediate situation in which the agent finds itself; i.e. a specific place and moment, an instantaneous configuration that puts the agent in relation to other significant things such as tools, obstacles, enemies or prizes. It can the current situation returned by the environment, or any future situation. Were you ever in the wrong place at the wrong time? That’s a state.\n",
        "### Reward (R)\n",
        "A reward is the feedback by which we measure the success or failure of an agent’s actions in a given state. For example, in a video game, when Mario touches a coin, he wins points. From any given state, an agent sends output in the form of actions to the environment, and the environment returns the agent’s new state (which resulted from acting on the previous state) as well as rewards, if there are any. Rewards can be immediate or delayed. They effectively evaluate the agent’s action.\n",
        "### Policy (π)\n",
        "The policy is the strategy that the agent employs to determine the next action based on the current state. It maps states to actions, the actions that promise the highest reward.\n",
        "### Value (V)\n",
        "The expected long-term return with discount, as opposed to the short-term reward R. Vπ(s) is defined as the expected long-term return of the current state under policy π. We discount rewards, or lower their estimated value, the further into the future they occur. See discount factor. And remember Keynes: “In the long run, we are all dead.” That’s why you discount future rewards. It is useful to distinguish\n",
        "### Q-value or action-value (Q)\n",
        "Q-value is similar to Value, except that it takes an extra parameter, the current action a. Qπ(s, a) refers to the long-term return of an action taking action a under policy π from the current state s. Q maps state-action pairs to rewards. Note the difference between Q and policy.\n",
        "### Trajectory \n",
        "A sequence of states and actions that influence those states. From the Latin “to throw across.” The life of an agent is but a ball tossed high and arching through space-time unmoored, much like humans in the modern world.\n",
        "### Key distinctions\n",
        "Reward is an immediate signal that is received in a given state, while value is the sum of all rewards you might anticipate from that state. Value is a long-term expectation, while reward is an immediate pleasure. Value is eating spinach salad for dinner in anticipation of a long and healthy life; reward is eating chocolate for dinner and to hell with it. They differ in their time horizons. \n",
        "\n",
        "##### So..\n",
        "You can have states where value and reward diverge: you might receive a low, immediate reward (spinach) even as you move to position with great potential for long-term value; or you might receive a high immediate reward (chocolate) that leads to diminishing prospects over time. This is why the value function, rather than immediate rewards, is what reinforcement learning seeks to predict and control.\n",
        "![](https://www.guru99.com/images/1/082319_0514_Reinforceme2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riv4L68dfNfC"
      },
      "source": [
        "# Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAtYDCWSnKip"
      },
      "source": [
        "Now we will develop a model of a self-driving car. \n",
        "\n",
        "Let's define the basic concepts in this stimulation:\n",
        "\n",
        "There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYG9-ISomWKC"
      },
      "source": [
        "**Rewards** \n",
        "\n",
        "Since the agent (the imaginary driver) is reward-motivated and is going to learn how to control the cab by trial experiences in the environment, we need to decide the rewards and/or penalties and their magnitude accordingly. Here a few points to consider:\n",
        "\n",
        "The agent should receive a high positive reward for a successful dropoff because this behavior is highly desired\n",
        "The agent should be penalized if it tries to drop off a passenger in wrong locations\n",
        "The agent should get a slight negative reward for not making it to the destination after every time-step. \"Slight\" negative because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raNiYmL0nLIY"
      },
      "source": [
        "**State space**\n",
        "\n",
        "The State Space is the set of all possible situations our taxi could inhabit. The state should contain useful information the agent needs to make the right action.\n",
        "\n",
        "We have a training area for our car. where we are teaching it to transport people in a parking lot to four different locations (R, G, Y, B):\n",
        "\n",
        "![](https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLSs7e7_nb-n"
      },
      "source": [
        "**Action Space**\n",
        "\n",
        "0 = south\n",
        "\n",
        "1 = north\n",
        "\n",
        "2 = east\n",
        "\n",
        "3 = west\n",
        "\n",
        "4 = pickup passanger\n",
        "\n",
        "5 = dropoff passanger\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aF6e9JQoFWd"
      },
      "source": [
        "#### Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3RNsJeSoHeh"
      },
      "source": [
        "Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH0AhqHOn7QP"
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UILQynI6oMg7"
      },
      "source": [
        "Import and environment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VasILGRJoTx2",
        "outputId": "3fe942eb-0c1b-41db-a569-895e1f106b80"
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYCSLluYpmIG"
      },
      "source": [
        "The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
        "The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
        "R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5usREjcIob2N"
      },
      "source": [
        "**env.reset**\n",
        "\n",
        "Resets the environment and returns a random initial state.\n",
        "\n",
        "**env.render**\n",
        "\n",
        "Renders one frame of the environment (helpful in visualizing the environment)\n",
        "\n",
        "**env.step(action)**\n",
        "\n",
        "Step the environment by one timestep. \n",
        "\n",
        "**Returns observation**\n",
        "\n",
        "Observations of the environment\n",
        "reward: \n",
        "\n",
        "If your action was beneficial or not\n",
        "done: Indicates if we have successfully picked up and dropped off a passenger, also called one episode\n",
        "\n",
        "info: \n",
        "Additional info such as performance and latency for debugging purposes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L4X_46Ipc15",
        "outputId": "ff06d397-ad99-4425-8818-e054d5ceb8b8"
      },
      "source": [
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G80Ln5jLrQ5I"
      },
      "source": [
        "**The Rewards table and Q-table**\n",
        "\n",
        "is a data structure used to calculate the maximum expected future reward for actions in each state. In fact, this table will help us do our best in each state. To study each value of the Q-table, the Q-Learning algorithm is used. It can be considered a matrix, where the number of states corresponds to the number of rows, and the number of actions corresponds to the number of columns.\n",
        "\n",
        "Since absolutely all states are recorded in this matrix, you can view the default reward values assigned to the state that we have selected for the illustration\n",
        "\n",
        "In our Taxi environment, we have the reward table, P, that the agent will learn from. It does thing by looking receiving a reward for taking an action in the current state, then updating a Q-value to remember if that action was beneficial.\n",
        "\n",
        "\n",
        "\n",
        "The values store in the Q-table are called a Q-values, and they map to a (state, action) combination.\n",
        "\n",
        "A Q-value for a particular state-action combination is representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards.\n",
        "\n",
        "For example, if the taxi is faced with a state that includes a passenger at its current location, it is highly likely that the Q-value for pickup is higher when compared to other actions, like dropoff or north.\n",
        "\n",
        "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbTQObyQr0J3"
      },
      "source": [
        "In this env, **probability** is always 1.0.\n",
        "\n",
        "The **nextstate** is the state we would be in if we take the action at this index of the dict\n",
        "\n",
        "All the movement actions have a -1 reward and the pickup/dropoff actions have -10 reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)\n",
        "**done** is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCyBuOETpgDS",
        "outputId": "2c5e49ce-7ca8-43e1-a2fb-687547aa6d72"
      },
      "source": [
        "env.P[328]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIXGdCIltFBZ"
      },
      "source": [
        "What happens if we perform actions randomly?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRDmvrXSrhYO",
        "outputId": "9782f077-747b-47f4-882d-b739be0a7099"
      },
      "source": [
        "env.s = 328  # set environment to illustration's state\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "frames = [] # for animation\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample() # random action\n",
        "    state, reward, done, info = env.step(action)\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    frames.append({\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "    epochs += 1\n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timesteps taken: 473\n",
            "Penalties incurred: 156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NELu6MgrhcV",
        "outputId": "682dd280-bce6-41b5-a5ec-54a0721248bb"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        env.s = frame['state']\n",
        "        env.render()\n",
        "        sleep(.1)\n",
        "        \n",
        "print_frames(frames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timestep: 473\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n",
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNblViEuyaTl"
      },
      "source": [
        "α is the degree to which our Q-values are updated at each iteration\n",
        "\n",
        "γ (gamma) how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy.\n",
        "We are assigning (←), or updating, the Q-value of the agent's current state and action by first taking a weight (1−α) of the old Q-value, then adding the learned value. The learned value is a combination of the reward for taking the current action in the current state, and the discounted maximum reward from the next state we will be in once we take the current action.\n",
        "\n",
        "Basically, we are learning the proper action to take in the current state by looking at the reward for the current state/action combo, and the max rewards for the next state. This will eventually cause our taxi to consider the route with the best rewards strung together.\n",
        "\n",
        "The Q-value of a state-action pair is the sum of the instant reward and the discounted future reward (of the resulting state). The way we store the Q-values for each state and action is through a Q-table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEVK0FbQyCMp"
      },
      "source": [
        "$Q(state,action) \\leftarrow (1−α)Q(state,action) + α(reward + γ\\max_{α}Q(next state, all\\,actions))$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAlI_dJEz-XR"
      },
      "source": [
        "The Q-table is a matrix where we have a row for every state (500) and a column for every action (6). It's first initialized to 0, and then values are updated after training. Note that the Q-table has the same dimensions as the reward table, but it has a completely different purpose.\n",
        "![](https://storage.googleapis.com/lds-media/images/q-matrix-initialized-to-learned_gQq0BFs.width-1200.png)\n",
        "Q-Table values are initialized to zero and then updated during training to values that optimize the agent's traversal through the environment for maximum rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILb-h2lS0K8Z"
      },
      "source": [
        "**Final instruction**\n",
        "\n",
        "Initialize the Q-table by all zeros.\n",
        "\n",
        "Start exploring actions: For each state, select any one among all possible actions for the current state (S).\n",
        "\n",
        "Travel to the next state (S') as a result of that action (a).\n",
        "\n",
        "For all possible actions from the state (S') select the one with the highest Q-value.\n",
        "\n",
        "Update Q-table values using the equation.\n",
        "\n",
        "Set the next state as the current state.\n",
        "If goal state is reached, then end and repeat the process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvzvkUEG0n04"
      },
      "source": [
        "**Exploiting learned values**\n",
        "\n",
        "After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.\n",
        "\n",
        "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called ϵ \"epsilon\" to cater to this during training.\n",
        "\n",
        "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further. Lower epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf2YK1U_0uNB"
      },
      "source": [
        "Initialize the Q-table to a 500×6 matrix of zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6ov4KJ9yVbj"
      },
      "source": [
        "import numpy as np\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAINovaX0-6d"
      },
      "source": [
        "\"While not done\", we decide whether to pick a random action or to exploit the already computed Q-values. This is done simply by using the epsilon value and comparing it to the random.uniform(0, 1) function, which returns an arbitrary number between 0 and 1.\n",
        "\n",
        "We execute the chosen action in the environment to obtain the next_state and the reward from performing the action. After that, we calculate the maximum Q-value for the actions corresponding to the next_state, and with that, we can easily update our Q-value to the new_q_value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "9e427c5a429e4a6b8c19db213894f93c",
            "bd0f5a4db2974a749ff95639480e8afc",
            "282add3a1d92467ab7913cec0a7d9af8",
            "9d8a8eb022a54aa6aa270b3eb61774db",
            "4a939888489045ecb6ce704e80e3f016",
            "4a8fa8cfa8334bc5af9e060087ff2119",
            "98277269b41e4d02a06a670f5ea7701b",
            "d91b613ca4f24eeca6c1f25822249624"
          ]
        },
        "id": "-N7gR57j0w9u",
        "outputId": "bee21ce1-65d3-44f3-80e7-d47861cf3e13"
      },
      "source": [
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in tqdm(range(1, 100001)):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() \n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) \n",
        "        \n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        \n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        \n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e427c5a429e4a6b8c19db213894f93c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training finished.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js41m-yi3H0J"
      },
      "source": [
        "Now that the Q-table has been established over 100,000 episodes, let's see what the Q-values are at our starting position"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzZOB3Zp2Pt2",
        "outputId": "e1d73aa0-dbea-4db1-f874-31aace7e4565"
      },
      "source": [
        "q_table[328]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.39595468,  -2.27325184,  -2.40612954,  -2.35721664,\n",
              "       -11.0696291 , -10.60909117])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeK0brxN3TIw"
      },
      "source": [
        "According to our agent, the best option is to go to north (-2.27325184)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu_A7vmE4yiR"
      },
      "source": [
        "We explored the area in the last section of the code. Now let's just run our agent based on the completed Q-table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cObcW2ot3gC8",
        "outputId": "23109659-aa8c-44e5-983c-487e322f01e8"
      },
      "source": [
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 13.48\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlgM5fRl5cRt"
      },
      "source": [
        "The result is significantly better in terms of the number of errors and the speed of the solution than if we tried to randomly perform actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxymgVKAnStw"
      },
      "source": [
        "# Microsoft Azure Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s3oXbhO9Xip"
      },
      "source": [
        "As part of the use of Microsoft Azure machine learning technologies, we will train our agent to play pong\n",
        "![](https://media0.giphy.com/media/xThuWtNFKZWG6fUFe8/giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNICU2Qc9Q3q"
      },
      "source": [
        "Installing the library and create a workspace configuration file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CukHdDx1hKlx"
      },
      "source": [
        "!pip install --upgrade azureml-core \n",
        "!pip install --upgrade azureml-contrib-reinforcementlearning\n",
        "!pip install azureml-sdk[notebooks]\n",
        "!pip install azureml-sdk azureml-tensorboard tensorflow\n",
        "!pip install -U ray\n",
        "!pip install utils\n",
        "!sudo apt-get install redis-server"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEPVWJzW91H3"
      },
      "source": [
        "# Download the file: In the Azure portal, \n",
        "# select Download config.json from the Overview \n",
        "# section of your workspace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGiCTSRg-UEP"
      },
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import Experiment\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "from azureml.core.runconfig import EnvironmentDefinition\n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.tensorboard import Tensorboard\n",
        "\n",
        "from azureml.contrib.train.rl import ReinforcementLearningEstimator, Ray\n",
        "from azureml.contrib.train.rl import WorkerConfiguration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c5erOtv-Zc2"
      },
      "source": [
        "# Don't forget to put the configuration file from the Azure portal\n",
        "ws = Workspace.from_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X543x9qAxns"
      },
      "source": [
        "experiment_name='pongs'\n",
        "\n",
        "exp = Experiment(workspace=ws, name=experiment_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-wVrTFrBc3A"
      },
      "source": [
        "# Create Virtual Network in Res.group\n",
        "vnet_name = 'vnetsa'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyv__LoGCaL_"
      },
      "source": [
        "Create Virtual Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps3ChPhMBc84",
        "outputId": "e66233e7-0113-41d3-f3e7-45d2f968e40b"
      },
      "source": [
        "from azureml.core.compute import AmlCompute, ComputeTarget\n",
        "# choose a name for your Ray worker cluster\n",
        "worker_compute_name = 'worker-cpu-sa'\n",
        "worker_compute_min_nodes = 0 \n",
        "worker_compute_max_nodes = 4\n",
        "\n",
        "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
        "worker_vm_size = 'STANDARD_D2_V2'\n",
        "\n",
        "# Create the compute target if it hasn't been created already\n",
        "if worker_compute_name in ws.compute_targets:\n",
        "    worker_compute_target = ws.compute_targets[worker_compute_name]\n",
        "    if worker_compute_target and type(worker_compute_target) is AmlCompute:\n",
        "        print(f'found worker compute target. just use it {worker_compute_name}')\n",
        "else:\n",
        "    print('creating a new worker compute target...')\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = worker_vm_size,\n",
        "                                                                min_nodes = worker_compute_min_nodes, \n",
        "                                                                max_nodes = worker_compute_max_nodes,\n",
        "                                                                vnet_resourcegroup_name = ws.resource_group,\n",
        "                                                                vnet_name = vnet_name,\n",
        "                                                                subnet_name = 'default')\n",
        "\n",
        "    # create the cluster\n",
        "    worker_compute_target = ComputeTarget.create(ws, worker_compute_name, provisioning_config)\n",
        "    \n",
        "    # can poll for a minimum number of nodes and for a specific timeout. \n",
        "    # if no min node count is provided it will use the scale settings for the cluster\n",
        "    worker_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "    \n",
        "     # For a more detailed view of current AmlCompute status, use get_status()\n",
        "    print(worker_compute_target.get_status().serialize())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating a new worker compute target...\n",
            "Creating...\n",
            "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n",
            "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-04-18T13:05:23.396000+00:00', 'errors': None, 'creationTime': '2021-04-18T13:05:20.642223+00:00', 'modifiedTime': '2021-04-18T13:05:36.418336+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4b7g1gTCyEu"
      },
      "source": [
        "Use the ReinforcementLearningEstimator to submit a training job to Azure Machine Learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lop0Uc1CPkM"
      },
      "source": [
        "# Pip packages we will use for both head and worker\n",
        "pip_packages = [\"ray[rllib]==0.8.3\"] # Latest version of Ray has fixes for isses related to object transfers\n",
        "\n",
        "# Specify the Ray worker configuration\n",
        "worker_conf = WorkerConfiguration(\n",
        "    # Azure ML compute cluster to run Ray workers\n",
        "    compute_target=worker_compute_target, \n",
        "    # Number of worker nodes\n",
        "    node_count=4,\n",
        "    # GPU\n",
        "    use_gpu=False, \n",
        "    # PIP packages to use\n",
        "    pip_packages=pip_packages\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F5dSFcYCz9Q"
      },
      "source": [
        "training_algorithm = \"IMPALA\"\n",
        "rl_environment = \"PongNoFrameskip-v4\"\n",
        "\n",
        "# Training script parameters\n",
        "script_params = {\n",
        "    \n",
        "    # Training algorithm, IMPALA in this case\n",
        "    \"--run\": training_algorithm,\n",
        "    \n",
        "    # Environment, Pong in this case\n",
        "    \"--env\": rl_environment,\n",
        "    \n",
        "    # Add additional single quotes at the both ends of string values as we have spaces in the \n",
        "    # string parameters, outermost quotes are not passed to scripts as they are not actually part of string\n",
        "    # Number of GPUs\n",
        "    # Number of ray workers\n",
        "    \"--config\": '\\'{\"num_gpus\": 1, \"num_workers\": 13}\\'',\n",
        "    \n",
        "    # Target episode reward mean to stop the training\n",
        "    # Total training time in seconds\n",
        "    \"--stop\": '\\'{\"episode_reward_mean\": 18, \"time_total_s\": 3600}\\'',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZooFARE5DhkC",
        "outputId": "f771afb3-f52f-44a1-f9fb-f406e3350f3b"
      },
      "source": [
        "# RL estimator\n",
        "rl_estimator = ReinforcementLearningEstimator(\n",
        "    \n",
        "    # Location of source files\n",
        "    source_directory='files',\n",
        "    \n",
        "    # Python script file\n",
        "    entry_script=\"pong_rllib.py\",\n",
        "    \n",
        "    # Parameters to pass to the script file\n",
        "    # Defined above.\n",
        "    script_params=script_params,\n",
        "    \n",
        "    # The Azure ML compute target set up for Ray head nodes\n",
        "    compute_target=head_compute_target,\n",
        "    \n",
        "    # Pip packages\n",
        "    pip_packages=pip_packages,\n",
        "    \n",
        "    # GPU usage\n",
        "    use_gpu=True,\n",
        "    \n",
        "    # RL framework. Currently must be Ray.\n",
        "    rl_framework=Ray(),\n",
        "    \n",
        "    # Ray worker configuration defined above.\n",
        "    worker_configuration=worker_conf,\n",
        "    \n",
        "    # How long to wait for whole cluster to start\n",
        "    cluster_coordination_timeout_seconds=3600,\n",
        "    \n",
        "    # Maximum time for the whole Ray job to run\n",
        "    # This will cut off the run after an hour\n",
        "    max_run_duration_seconds=3600,\n",
        "    \n",
        "    # Allow the docker container Ray runs in to make full use\n",
        "    # of the shared memory available from the host OS.\n",
        "    shm_size=24*1024*1024*1024\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n",
            "'shm_size' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object instead.\n",
            "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiFd5vNdEMUs"
      },
      "source": [
        "The entry script pong_rllib.py trains a neural network using the OpenAI Gym environment PongNoFrameSkip-v4. OpenAI Gyms are standardized interfaces to test reinforcement learning algorithms on classic Atari games.\n",
        "\n",
        "This example uses a training algorithm known as IMPALA (Importance Weighted Actor-Learner Architecture). IMPALA parallelizes each individual learning actor to scale across many compute nodes without sacrificing speed or stability.\n",
        "\n",
        "Ray Tune orchestrates the IMPALA worker tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRASCreCGFSO"
      },
      "source": [
        "import ray\n",
        "import ray.tune as tune\n",
        "from ray.rllib import train\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from azureml.core import Run\n",
        "from utils import callbacks\n",
        "\n",
        "DEFAULT_RAY_ADDRESS = 'localhost:6379'\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Parse arguments\n",
        "    train_parser = train.create_parser()\n",
        "\n",
        "    args = train_parser.parse_args()\n",
        "    print(\"Algorithm config:\", args.config)\n",
        "\n",
        "    if args.ray_address is None:\n",
        "        args.ray_address = DEFAULT_RAY_ADDRESS\n",
        "\n",
        "    ray.init(address=args.ray_address)\n",
        "\n",
        "    tune.run(run_or_experiment=args.run,\n",
        "             config={\n",
        "                 \"env\": args.env,\n",
        "                 \"num_gpus\": args.config[\"num_gpus\"],\n",
        "                 \"num_workers\": args.config[\"num_workers\"],\n",
        "                 \"callbacks\": {\"on_train_result\": callbacks.on_train_result},\n",
        "                 \"sample_batch_size\": 50,\n",
        "                 \"train_batch_size\": 1000,\n",
        "                 \"num_sgd_iter\": 2,\n",
        "                 \"num_data_loader_buffers\": 2,\n",
        "                 \"model\": {\n",
        "                    \"dim\": 42\n",
        "                 },\n",
        "             },\n",
        "             stop=args.stop,\n",
        "             local_dir='./logs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtQhL0HoElqN"
      },
      "source": [
        "from azureml.core import Run\n",
        "\n",
        "def on_train_result(info):\n",
        "    run = Run.get_context()\n",
        "    run.log(\n",
        "        name='episode_reward_mean',\n",
        "        value=info[\"result\"][\"episode_reward_mean\"])\n",
        "    run.log(\n",
        "        name='episodes_total',\n",
        "        value=info[\"result\"][\"episodes_total\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co6M9QavLmb9"
      },
      "source": [
        "Submit a run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn9sz1r3LhNq"
      },
      "source": [
        "run = exp.submit(config=rl_estimator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvvkZglcLlM7"
      },
      "source": [
        "Monitor and view results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo-SmHS9Li55"
      },
      "source": [
        "from azureml.widgets import RunDetails\n",
        "\n",
        "RunDetails(run).show()\n",
        "run.wait_for_completion()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS0jWfSOL0Yt"
      },
      "source": [
        "### Final Result \n",
        "![](https://docs.microsoft.com/en-us/azure/machine-learning/media/how-to-use-reinforcement-learning/pong-run-details-widget.png)"
      ]
    }
  ]
}